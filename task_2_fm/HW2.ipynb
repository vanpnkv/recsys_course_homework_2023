{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":295801,"sourceType":"datasetVersion","datasetId":122667},{"sourceId":7075838,"sourceType":"datasetVersion","datasetId":4075551}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install xlearn","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:25:50.926866Z","iopub.execute_input":"2023-12-03T10:25:50.927327Z","iopub.status.idle":"2023-12-03T10:27:27.849645Z","shell.execute_reply.started":"2023-12-03T10:25:50.927292Z","shell.execute_reply":"2023-12-03T10:27:27.847785Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting xlearn\n  Downloading xlearn-0.40a1.tar.gz (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: xlearn\n  Building wheel for xlearn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for xlearn: filename=xlearn-0.40a1-py3-none-any.whl size=225729 sha256=fb1fb8045f0f548e1520c56cae744dd5429dafc2eb48942c8350295fbe65c417\n  Stored in directory: /root/.cache/pip/wheels/09/48/04/779ee06b22532c86cde8da8984b83284517492dad1df998c6a\nSuccessfully built xlearn\nInstalling collected packages: xlearn\nSuccessfully installed xlearn-0.40a1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss, roc_auc_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom datetime import datetime\nfrom sklearn.preprocessing import OneHotEncoder\nimport scipy\nfrom sklearn.model_selection import cross_validate\nimport xlearn as xl\nimport os\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:27:27.852220Z","iopub.execute_input":"2023-12-03T10:27:27.852940Z","iopub.status.idle":"2023-12-03T10:27:28.009748Z","shell.execute_reply.started":"2023-12-03T10:27:27.852894Z","shell.execute_reply":"2023-12-03T10:27:28.008226Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"os.environ['USER'] = 'test'","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:27:28.011430Z","iopub.execute_input":"2023-12-03T10:27:28.011811Z","iopub.status.idle":"2023-12-03T10:27:28.017972Z","shell.execute_reply.started":"2023-12-03T10:27:28.011781Z","shell.execute_reply":"2023-12-03T10:27:28.016604Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_date_features(x):\n    x_datetime = datetime.strptime(x.split('.')[0], '%Y-%m-%d %H:%M:%S')\n    year = x_datetime.year\n    month = x_datetime.month\n    day = x_datetime.day\n    hour = x_datetime.hour\n    \n    return [year, month, day, hour]\n\n\ndef split_date_time(data: pd.DataFrame):\n    date_values= np.stack(data['date_time'].apply(lambda x: get_date_features(x)).values)\n    data['year'] = date_values[:, 0]\n    data['month'] = date_values[:, 1]\n    data['day'] = date_values[:, 2]\n    data['hour'] = date_values[:, 3]\n    data = data.drop(columns=['date_time'])\n    return data\n\n\ndef feature_engineering(data: pd.DataFrame) -> pd.DataFrame:    \n    # Разобью столбец date_time на год/месяц/день/час\n    # Минуты и секунды я дропаю, тк кажется, что эти значения не могут нести полезной информации\n    data = split_date_time(data)\n    \n    # Удалю константные фичи\n    data = data.drop(columns=['impressions', 'year'])\n    last_day_indices = np.logical_and((data['month'] == 10).values, (data['day'] == 2).values)\n    train_data, test_data = data[np.logical_not(last_day_indices)], data[last_day_indices]\n    \n    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n    \n    return train_data, val_data, test_data","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:27:28.020952Z","iopub.execute_input":"2023-12-03T10:27:28.021420Z","iopub.status.idle":"2023-12-03T10:27:28.039466Z","shell.execute_reply.started":"2023-12-03T10:27:28.021359Z","shell.execute_reply":"2023-12-03T10:27:28.037362Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"all_data = pd.read_csv('/kaggle/input/recsys/data.csv')\nall_data = all_data.drop(columns=['banner_id0', 'banner_id1',\\\n                              'rate0', 'rate1', 'g0', 'g1', 'coeff_sum0', 'coeff_sum1'])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:27:28.042290Z","iopub.execute_input":"2023-12-03T10:27:28.043395Z","iopub.status.idle":"2023-12-03T10:28:47.706652Z","shell.execute_reply.started":"2023-12-03T10:27:28.043334Z","shell.execute_reply":"2023-12-03T10:28:47.705307Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df, val_df, test_df = feature_engineering(all_data.sample(100000))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:28:47.709682Z","iopub.execute_input":"2023-12-03T10:28:47.710506Z","iopub.status.idle":"2023-12-03T10:28:51.052485Z","shell.execute_reply.started":"2023-12-03T10:28:47.710451Z","shell.execute_reply":"2023-12-03T10:28:51.050843Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Определение категориальных признаков\ncategorical_features = [\"zone_id\", \"banner_id\", \"oaid_hash\", \"os_id\", \"country_id\", \"hour\", \"month\", \"day\"]\n\n# Преобразование категориальных признаков\nfield_dict = {col: i for i, col in enumerate(categorical_features)}\ncategory_maps = {col: {val: i for i, val in enumerate(pd.concat((train_df, val_df, test_df))[col].unique())} for col in categorical_features}\n\ndef to_libffm(df, target, save_path):\n    def convert_to_ffm(row):\n        ffm_row = [str(row[target])]  # target\n        for col in categorical_features:\n            field_id = field_dict[col]\n            category_id = category_maps[col][row[col]]\n            ffm_row.append(f\"{field_id}:{category_id}:1\")\n        return ' '.join(ffm_row)\n\n    # Преобразование DataFrame и сохранение в файл\n    ffm_data = df.apply(convert_to_ffm, axis=1)\n    ffm_data.to_csv(save_path, index=False, header=False, sep='\\n')\n\nto_libffm(train_df, 'clicks', 'train.txt')\nto_libffm(val_df, 'clicks', 'val.txt')\nto_libffm(test_df, 'clicks', 'test.txt')","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:50:19.166648Z","iopub.execute_input":"2023-12-03T10:50:19.167130Z","iopub.status.idle":"2023-12-03T10:50:26.637217Z","shell.execute_reply.started":"2023-12-03T10:50:19.167094Z","shell.execute_reply":"2023-12-03T10:50:26.635214Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:29:00.634558Z","iopub.execute_input":"2023-12-03T10:29:00.636879Z","iopub.status.idle":"2023-12-03T10:29:00.664616Z","shell.execute_reply.started":"2023-12-03T10:29:00.636802Z","shell.execute_reply":"2023-12-03T10:29:00.663006Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"y_val = val_df['clicks'].values\n\nroc_auc_scores = []\nlog_losses = []\n\nfor l in [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]:\n    for k in [2, 4, 8, 16, 32]:\n        ffm_model = xl.create_ffm()\n        ffm_model.setTrain(\"train.txt\")\n        ffm_model.setTest(\"val.txt\")\n        param = {'task':'binary', 'lr': 0.1, 'lambda': l, 'k': k, 'metric': 'auc'}\n\n        ffm_model.fit(param, './model.out')\n        ffm_model.setSigmoid()\n        ffm_model.predict('./model.out', './rusult.txt')\n        \n        with open('rusult.txt', 'r') as f:\n            y_pred_proba = np.array(list(map(float, filter(lambda s: len(s) > 0, f.read().split('\\n')))))\n        roc_auc_scores.append([l, k, roc_auc_score(y_val, y_pred_proba)])\n        log_losses.append([l, k, log_loss(y_val, y_pred_proba)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argmax(np.array(roc_auc_scores)[:, 2])\nroc_auc_scores[13]","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:57:13.026088Z","iopub.execute_input":"2023-12-03T10:57:13.026557Z","iopub.status.idle":"2023-12-03T10:57:13.038679Z","shell.execute_reply.started":"2023-12-03T10:57:13.026525Z","shell.execute_reply":"2023-12-03T10:57:13.036153Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[0.0001, 16, 0.738948523838858]"},"metadata":{}}]},{"cell_type":"code","source":"train_df, val_df, test_df = feature_engineering(all_data)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:58:20.046843Z","iopub.execute_input":"2023-12-03T10:58:20.047376Z","iopub.status.idle":"2023-12-03T11:04:30.948104Z","shell.execute_reply.started":"2023-12-03T10:58:20.047337Z","shell.execute_reply":"2023-12-03T11:04:30.945799Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"field_dict = {col: i for i, col in enumerate(categorical_features)}\ncategory_maps = {col: {val: i for i, val in enumerate(pd.concat((train_df, val_df, test_df))[col].unique())} for col in categorical_features}\n\ntrain_df = pd.concat((train_df, val_df))\n\nto_libffm(train_df, 'clicks', 'train.txt')\nto_libffm(test_df, 'clicks', 'test.txt')\n\ny_test = test_df['clicks'].values\n\nffm_model = xl.create_ffm()\nffm_model.setTrain(\"train.txt\")\nffm_model.setTest(\"test.txt\")\nparam = {'task':'binary', 'lr': 0.1, 'lambda': 0.0001, 'k': 16, 'metric': 'auc'}\n\nffm_model.fit(param, './model.out')\nffm_model.setSigmoid()\nffm_model.predict('./model.out', './rusult.txt')\n\nwith open('rusult.txt', 'r') as f:\n    y_pred_proba = np.array(list(map(float, filter(lambda s: len(s) > 0, f.read().split('\\n')))))\n\n    roc_auc_metric = roc_auc_score(y_test, y_pred_proba)\nlog_loss_metric = log_loss(y_test, y_pred_proba)\nprint(f'roc_auc = {roc_auc_metric}, log_loss = {log_loss_metric}')","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:06:14.155794Z","iopub.execute_input":"2023-12-03T11:06:14.156239Z","iopub.status.idle":"2023-12-03T11:39:31.393011Z","shell.execute_reply.started":"2023-12-03T11:06:14.156208Z","shell.execute_reply":"2023-12-03T11:39:31.390979Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n           _\n          | |\n     __  _| |     ___  __ _ _ __ _ __\n     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n      >  <| |___|  __/ (_| | |  | | | |\n     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n\n        xLearn   -- 0.40 Version --\n----------------------------------------------------------------------------------------------\n\n\u001b[39m\u001b[0m\u001b[35m\u001b[1m[ WARNING    ] Validation file not found, xLearn has already disable early-stopping.\u001b[0m\n\u001b[35m\u001b[1m[ WARNING    ] Validation file not found, xLearn has already disable (-x auc) option.\u001b[0m\n\u001b[32m[------------] \u001b[0mxLearn uses 4 threads for training task.\n\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n\u001b[32m[------------] \u001b[0mBinary file (train.txt.bin) NOT found. Convert text file to binary file.\n\u001b[32m[------------] \u001b[0mNumber of Feature: 5660418\n\u001b[32m[------------] \u001b[0mNumber of Field: 8\n\u001b[32m[------------] \u001b[0mTime cost for reading problem: 58.99 (sec)\n\u001b[32m\u001b[1m[ ACTION     ] Initialize model ...\u001b[0m\n\u001b[32m[------------] \u001b[0mModel size: 5.44 GB\n\u001b[32m[------------] \u001b[0mTime cost for model initial: 10.38 (sec)\n\u001b[32m\u001b[1m[ ACTION     ] Start to train ...\u001b[0m\n\u001b[32m[------------]\u001b[0m Epoch      Train log_loss     Time cost (sec)\n\u001b[32m[ \u001b[0m  10%\u001b[32m      ]\u001b[0m     1            0.103542               28.98\n\u001b[32m[ \u001b[0m  20%\u001b[32m      ]\u001b[0m     2            0.099787               28.75\n\u001b[32m[ \u001b[0m  30%\u001b[32m      ]\u001b[0m     3            0.096293               28.82\n\u001b[32m[ \u001b[0m  40%\u001b[32m      ]\u001b[0m     4            0.091405               28.89\n\u001b[32m[ \u001b[0m  50%\u001b[32m      ]\u001b[0m     5            0.084731               29.01\n\u001b[32m[ \u001b[0m  60%\u001b[32m      ]\u001b[0m     6            0.077008               28.86\n\u001b[32m[ \u001b[0m  70%\u001b[32m      ]\u001b[0m     7            0.069269               28.91\n\u001b[32m[ \u001b[0m  80%\u001b[32m      ]\u001b[0m     8            0.062341               28.93\n\u001b[32m[ \u001b[0m  90%\u001b[32m      ]\u001b[0m     9            0.056641               28.88\n\u001b[32m[ \u001b[0m 100%\u001b[32m      ]\u001b[0m    10            0.051920               28.88\n\u001b[32m\u001b[1m[ ACTION     ] Start to save model ...\u001b[0m\n\u001b[32m[------------] \u001b[0mModel file: ./model.out\n\u001b[32m[------------] \u001b[0mTime cost for saving model: 28.23 (sec)\n\u001b[32m\u001b[1m[ ACTION     ] Finish training\u001b[0m\n\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n\u001b[32m\u001b[1m[------------] Total time cost: 387.00 (sec)\u001b[0m\n\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n           _\n          | |\n     __  _| |     ___  __ _ _ __ _ __\n     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n      >  <| |___|  __/ (_| | |  | | | |\n     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n\n        xLearn   -- 0.40 Version --\n----------------------------------------------------------------------------------------------\n\n\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 4 threads for prediction task.\n\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n\u001b[32m[------------] \u001b[0mLoad model from ./model.out\n\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n\u001b[32m[------------] \u001b[0mScore function: ffm\n\u001b[32m[------------] \u001b[0mNumber of Feature: 5660418\n\u001b[32m[------------] \u001b[0mNumber of K: 16\n\u001b[32m[------------] \u001b[0mNumber of field: 8\n\u001b[32m[------------] \u001b[0mTime cost for loading model: 46.06 (sec)\n\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n\u001b[32m[------------] \u001b[0mBinary file (test.txt.bin) NOT found. Convert text file to binary file.\n\u001b[32m[------------] \u001b[0mTime cost for reading problem: 5.54 (sec)\n\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n\u001b[32m[------------] \u001b[0mThe test loss is: 0.133153\n\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n\u001b[32m\u001b[1m[------------] Total time cost: 53.80 (sec)\u001b[0m\nroc_auc = 0.7877827249206713, log_loss = 0.13312216860809437\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Итоговый скор: roc_auc = 0.7877827249206713, log_loss = 0.13312216860809437 \nСкор из предыдущего дз: roc_auc = 0.7225271872729148, log_loss = 0.7550432223404697\n\nРезультат получилось улучшить.","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:05:07.434221Z","iopub.execute_input":"2023-12-03T11:05:07.435545Z","iopub.status.idle":"2023-12-03T11:05:07.442025Z","shell.execute_reply.started":"2023-12-03T11:05:07.435496Z","shell.execute_reply":"2023-12-03T11:05:07.440570Z"}}}]}